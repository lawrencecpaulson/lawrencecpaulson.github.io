---
layout: post
title:  Everything you know is wrong
usemathjax: true 
tags: [general, memories]
---
Among the few advantages of attaining the dizzy age of 70
is the ability to look back on half a century.
Things were great back in 1975.
The Cold War was over, thanks to the détente established by Nixon and Brezhnev.
Most of the world's population lived under Marxism,
with even Rolls-Royce a state-run enterprise.
The world of fashion discovered pastels, bell bottoms, hot pants and platform shoes:
the very zenith of human culture, styles that would live forever.
Great progress had been made in Artificial Intelligence,
thanks to a relentless focus on symbol processing 
as opposed to discredited, useless neural networks.
Many thought that automatic theorem provers could 
lead the way to what we now call AGI.
Well, everything looks different now.

### Artificial Intelligence

AI is an extreme example of changing research trends.
Already in 1961, a computer program had beat MIT students at calculus problems.
By 1970, Terry Winograd's [SHRDLU](https://mitmuseum.mit.edu/collections/object/2007.020.027) demonstrated something 
remarkably like sentience in its use of English.
The sort of wild predictions made today by AI boosters –
that machines would put everybody out of work and maybe kill us all –
were already being made back then.
Consider the movie *2001: A Space Odyssey*, which was released in 1968,
featuring the lethal and highly intelligent HAL 9000.
Director Stanley Kubrick had chosen MIT's Marvin Minsky, 
possibly the world's leading AI researcher,
to be his scientific advisor.
Minsky's prediction turned out to be way off.

All AI work in that era was based on manipulating symbols.
The book *Perceptrons*, by Minsky and Seymour Papert,
had shown that neural networks could not even learn
such a simple function as exclusive-or: they were worthless.
The remaining debate was whether intelligence was *procedural*
(embodied in code) or *declarative* (encoded in a symbolic formalism).
SHRDLU was an example of procedural knowledge, 
a great mass of LISP code that no one was able to adapt for other purposes.
Declarative knowledge might be expressed in first-order logic 
or some other approach to *knowledge representation*, 
processed by an automatic theorem prover or other engine.

Expert systems were attracting widespread attention by 1980.
These contained declarative knowledge in the form of *production rules* 
obtained by interrogating actual experts in a specific domain. 
For example, [MYCIN](https://doi.org/10.1016/S0020-7373(78)80049-2) 
could prescribe antibiotics based on a description of symptoms.
It could even "explain" its decisions 
based on the production rules involved in the response to a query.

By the late 1980s, disenchantment with expert systems and other symbolic approaches led to an ["AI Winter"](https://dl.acm.org/doi/10.1145/3625833)
that did not really end until neural networks were revived.

### Specialised hardware for declarative languages

[SKIM machine](https://dl.acm.org/doi/10.1145/800087.802798)

[Transputer](http://people.cs.bris.ac.uk/~dave/transputer.html)

Standard ML, Haskell, Prolog

all crushed by Intel

### DPLL versus resolution theorem proving

Interest in automatic theorem proving dates back at least 1956:
[Logic Theorist](https://www.historyofinformation.com/detail.php?id=742)
could prove some simple statements from Whitehead and Russell's
*Principia Mathematica*.
[Hao Wang](https://lawrencecpaulson.github.io/tag/Hao_Wang) and others were excited by the possibility of computers
automating the process of mathematical proof.
One outcome of such work was the 
[DPLL procedure](https://doi.org/10.1145/368273.368557) for propositional logic:
the first *SAT solver*.
It was quickly superseded by the [resolution procedure](https://doi.org/10.1145/321250.321253),
which worked in the richer formalism of first-order logic.
By the mid 1970s, people were even getting 
[disenchanted with resolution](https://dl.acm.org/doi/abs/10.1016/0004-3702(77)90012-1),
but DPLL still seemed as dead as a doornail.
Attempts to revive DPLL during the 1990s provoked laughter in some,
but nobody is laughing now. 
Resolution has also grown in importance, 
but nobody thinks it can do robot planning anymore.
[Prolog](https://www.usaii.org/ai-insights/what-is-prolog-programming-language-an-overview), 
a programming language originating in a specialised form of resolution,
attracted enormous interest in the 1980s but had since fallen by the wayside.

Today, DPLL is the basis of powerful theorem provers 
[based on SMT solvers](https://doi.org/10.1145/3587692) 
and used for verifying software.
Both SMT and resolution are used every time an Isabelle user invokes
[Sledgehammer](https://www.cl.cam.ac.uk/~lp15/papers/Automation/iwil2010-sledgehammer.pdf).

### Program verification versus correct-program synthesis

*Program verification* is the process of proving correctness properties
of code that has already been written, identifying defects in the process.
For much of my career the very idea has been [heavily criticised]({% post_url 2025-03-14-revisiting_demillo %}),
mostly as being infeasible[^1] but also as being back-to-front.
Begin with the problem specification, people said, 
and work backwards from there to code that is correct by design.

[^1]: My dictionary actually illustrates the definition of *infeasible* with the sentence "proof that a program works is infeasible unless it is very short".

One of the foremost proponents of that approach was Edgar W Dijkstra,
a dominant figure right up until his death in 2002.
Already in his 1972 essay "[Notes on structured programming](https://dl.acm.org/doi/10.5555/1243380.1243381)"
he set himself the task of printing a table of 1000 prime numbers,
spending 11 pages refining the task "print first thousand prime numbers"
to pseudocode in a language resembling [Algol W](https://bitsavers.org/pdf/stanford/cs_techReports/STAN-CS-71-230_Algol_W_Reference_Manual_Feb72.pdf).
In this example he did not write his specifications in logic,
but in further years his developments became increasingly formal.
One of his bugbears was what he called a *rabbit*:
an idea simply "pulled out of a hat" rather than determined by a calculation.
I find it odd to be against having ideas.

A formal and rather elegant "[deductive approach to program synthesis](https://doi.org/10.1145/357084.357090)"
was set forth by Zohar Manna and Richard Waldinger.
They used a tableau style formalism intermingling logical formulas
with code fragments and with a set of inference rules governing the translation
from logic to executable code.
Even more elegant and formal were 
the [constructive type theories](http://www.jstor.org/stable/37448) 
outlined by Per Martin-Löf.
By formulating logical [propositions as types]({% post_url 2023-08-23-Propositions_as_Types %}), such theories
guaranteed that the proof of a given statement yielded a *proof object*:
executable code witnessing the claim. The approach required
[constructive logic]({% post_url 2022-04-20-Why-constructive %}),
but the loss of classical reasoning was a small price to pay
if you got bug-free code for free.

**If**. This dream could not work for a number of reasons:

* Not all code has the same performance. It is easier to write the code you want and prove it afterwards than to prove a theorem in the right way to generate the code you want.
* [Specifications are seldom]({% post_url 2025-09-05-All_or_nothing %}) stable or complete. We know how to extend a program proof to cover additional properties, but we don't have a deductive synthesis approach that is open to adding further properties later.
* You are confined to constructive reasoning even when proving properties unrelated to the code. Many proof objects are actually [irrelevant]({% post_url 2023-08-23-Propositions_as_Types %}).

To see the first point, imagine synthesising a sorting function.
We have to prove that every finite sequence of integers
can be permuted into non-decreasing order.
There is a simple proof by induction: swap the smallest element of the sequence
with the first element, and since the remaining sequence is
shorter by one, the conclusion follows.
The corresponding algorithm is *selection sort*, 
which is no good: it takes quadratic time.
To write a proof that corresponds to quicksort,
you have to know quicksort already.
Even in Dijkstra's 1972 essay, he begins by remarking that
the most efficient way to generate prime numbers is the Sieve of Eratosthenes"
and concludes by noting that the code he derived much later was indeed
the Sieve of Eratosthenes.

Related was the field of *program transformations*, 
exemplified by the work of [Richard Bird](https://en.wikipedia.org/wiki/Richard_Bird_(computer_scientist)).
Working in the functional programming framework, the original specification
would actually be executable but inefficient.
Your task would be to transform this initial program 
into something more efficient.

I am one of the many people who were attracted to type theory by the dream 
of deductive program synthesis in the early 1980s.
Every so often, some researcher would attempt to execute 
the proof object they had derived from some interesting theorem.
In almost every case, their extracted code turned out to be all but unusable.
When I checked Wikipedia today, their entry on [Program Synthesis](https://en.wikipedia.org/wiki/Program_synthesis)
mentioned the work of Manna and Waldinger but did not contain
a single mention of the words "type" or "constructive". Dang.

### Postscript

I wish I could say that I had taken my own advice, ignored the herd
and struck out boldly on my own.
