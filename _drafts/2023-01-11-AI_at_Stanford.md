---
layout: post
title:  "Memories: artificial intelligence at Stamford in the 70s"
usemathjax: true 
tags: [memories]
---

These days, artificial intelligence (AI) is synonymous with machine learning (ML),
but old-timers can remember when the symbolic approach was king,
with the wonderful acronym [GOFAI](https://en.wikipedia.org/wiki/GOFAI).
I was at Stanford University from 1977 until 1982, and can vividly recall
the unique atmosphere of the Stanford AI Lab at that time.

### AI in the 1960s

My own interest in AI (as a high school student in the USA) arose
from references to some of MIT's luminaries in Martin Gardner's
"[Mathematical Games](https://en.wikipedia.org/wiki/List_of_Martin_Gardner_Mathematical_Games_columns)" column 
in *Scientific American*
and to a hagiographic article in *National Geographic*, 
also focused on MIT. I also saw a [film containing a demo](https://youtu.be/bo4RvYJYOzI)
of Terry Winograd's [SHRDLU](https://hci.stanford.edu/~winograd/shrdlu/).
The dialogue that Winograd carried out with his robot 
about blocks in a virtual world  gave a strong impression of sentience.
[Winograd himself later admitted](https://en.wikipedia.org/wiki/SHRDLU)
was largely bogus:

> Pressure was for something you could demo. ... I think AI suffered from that a lot, because it led to "Potemkin villages", things which - for the things they actually did in the demo looked good, but when you looked behind that there wasn't enough structure to make it really work more generally.

The film *2001: A Space Odyssey* had come out in 1968, envisaging
that within 32 years, computers would be capable of carrying out intelligent dialogue. One of its scientific advisers was MIT's
[Marvin Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky),
the de facto leader of the AI community.

I had the opportunity to enter the field, having been offered an 
undergraduate place at MIT to start in 1973.
Bizarrely, given my enthusiasm for AI at that time, I instead took up
an offer from Caltech. 
Little AI was done, and 1973 also saw the highly critical
[Lighthill Report](http://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/p001.htm), 
which led to [severe funding cuts](https://en.wikipedia.org/wiki/AI_winter) in both the USA and the UK.

### Stanford, 1977

In the 1970s, the Stanford AI Lab (SAIL) was located some distance
from the main campus, in the DC Power Building, a wooden semicircular structure that was slowly decaying. I can't recall why I usthere
go there, since I wasn't doing AI. But it was worth the half hour bike 
ride.

It had one of the world's first laser printers, then called the XGP
(Xerox Graphics Printer), the size of a refrigerator, which printed
On some sort of thermal paper that came in rolls. It cut the paper
into pages using an automatic guillotine, and the last page was typically
a little strip (why waste paper?). The computer, some sort of PDP 10,
ran a bespoke operating system based on an old version of DEC's
operating system.


 

[survey of early AI](https://projects.csail.mit.edu/films/aifilms/AIFilms.html)


### Postscript

When I arrived at the computer laboratory in 1982, its Head
was Prof Roger Needham. I was amused to stumble upon 
[some comments](http://www.chilton-computing.org.uk/inf/literature/reports/lighthill_report/p003.htm) 
he wrote in response to the Lighthill Report.

> Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.

The term AI still refers to a very mixed bunch of things.
